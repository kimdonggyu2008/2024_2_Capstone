{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/2024_2_Capstone/blob/main/%EC%9A%94%EC%95%BD%EB%AA%A8%EB%8D%B8%EA%B3%BC_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B_%ED%95%99%EC%8A%B5_%EC%BD%94%EB%93%9C(%EA%B9%80%EC%97%B0%ED%9B%88).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "7RXEWO1ccBg_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI-FEIIuUlRC",
        "outputId": "990af057-08b6-450d-e15b-a64dceff144d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8Iiq0LkzUCUT"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "58aw0GJxUjm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델과 데이터셋 설정\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "oA2DD-2SU5sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로드 (Huggingface의 CNN/DailyMail 데이터셋 사용)\n",
        "dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")"
      ],
      "metadata": {
        "id": "a7RgLSNYVDn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋의 article 항목 최대 길이 및 90% 표준편차 길이 확인\n",
        "article_lengths = [len(article) for article in dataset[\"train\"][\"article\"]]\n",
        "max_article_length = max(article_lengths)\n",
        "length_90_percentile = int(np.percentile(article_lengths, 95))\n",
        "print(f\"Maximum article length in dataset: {max_article_length} characters\")\n",
        "print(f\"90% percentile article length in dataset: {length_90_percentile} characters\")"
      ],
      "metadata": {
        "id": "iSTVRgvmKaEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processed_dataset_path = \"/content/drive/MyDrive/Colab Notebooks/Pegasus_요약모델/tokenized_datasets.pkl\"\n",
        "\n",
        "# # 데이터 전처리 함수, 토크나이징, 패딩\n",
        "# def preprocess_function(examples):\n",
        "#     inputs = examples[\"article\"]\n",
        "#     model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     # 레이블 설정\n",
        "#     with tokenizer.as_target_tokenizer():\n",
        "#         labels = tokenizer(examples[\"highlights\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs"
      ],
      "metadata": {
        "id": "RzFF8WSrVqmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset_path = \"/content/drive/MyDrive/summarizer/preprcessed/preprocessed_dataset.pkl\"\n",
        "data_folder_path = \"/content/drive/MyDrive/summarizer/data\""
      ],
      "metadata": {
        "id": "FWJsAuadLctB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"article\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=8000, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # 레이블 설정\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"highlights\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # 레이블에서 패딩 토큰을 -100으로 변경하여 로스 계산에서 무시하도록 설정\n",
        "    if isinstance(labels[\"input_ids\"], torch.Tensor):\n",
        "        labels[\"input_ids\"] = labels[\"input_ids\"].masked_fill(labels[\"input_ids\"] == tokenizer.pad_token_id, -100)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "wE56zvfnOPe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes = []\n",
        "for filename in os.listdir(data_folder_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(data_folder_path, filename)\n",
        "        df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "        # 특정 열 이름만 바꾸기\n",
        "        if filename == \"news_summary.csv\":\n",
        "            df = df.rename(columns={\n",
        "                \"ctext\": \"article\",  # 'ctext'를 'article'로 변경\n",
        "                \"text\": \"highlights\"  # 'text'를 'highlights'로 변경\n",
        "            })\n",
        "\n",
        "        elif filename == \"news_summary_more.csv\":\n",
        "            df = df.rename(columns={\n",
        "                \"headlines\": \"highlights\",  # 'headlines'를 'highlights'로 변경\n",
        "                \"text\": \"article\"  # 'text'를 'article'로 변경\n",
        "            })\n",
        "\n",
        "        # 변경된 파일 다시 저장\n",
        "        df.to_csv(file_path, index=False)\n",
        "\n",
        "        # 데이터프레임 리스트에 추가\n",
        "        dataframes.append(df)"
      ],
      "metadata": {
        "id": "sO_qvSVMKLWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(processed_dataset_path):\n",
        "    try:\n",
        "        print(\"Loading preprocessed dataset from file...\")\n",
        "        with open(processed_dataset_path, \"rb\") as f:\n",
        "            tokenized_datasets = pickle.load(f)\n",
        "        print(\"Preprocessed dataset loaded successfully.\")\n",
        "        start_index = len(tokenized_datasets)\n",
        "    except (pickle.UnpicklingError, EOFError) as e:\n",
        "        print(\"Error loading preprocessed dataset. Starting from scratch...\")\n",
        "        tokenized_datasets = []\n",
        "        start_index = 110000\n",
        "else:\n",
        "    print(\"No preprocessed dataset found. Starting from scratch...\")\n",
        "    tokenized_datasets = []\n",
        "    start_index = 110000\n",
        "\n",
        "print(\"Preprocessing dataset from index {}...\".format(start_index))\n",
        "for index in range(start_index, len(dataset[\"train\"])):\n",
        "    tokenized_data = preprocess_function(dataset[\"train\"][index])\n",
        "    tokenized_datasets.append(tokenized_data)\n",
        "    # 중간중간 저장하여 RAM 용량 절약\n",
        "    if (index + 1) % 10000 == 0:  # 매 10000개씩 기존 파일에 추가 저장\n",
        "        with open(processed_dataset_path, \"wb\") as f:\n",
        "            pickle.dump(tokenized_datasets, f)\n",
        "        print(f\"Saved {index + 1} tokenized examples so far...\")\n",
        "\n",
        "# 최종 저장\n",
        "with open(processed_dataset_path, \"wb\") as f:\n",
        "    pickle.dump(tokenized_datasets, f)\n",
        "print(\"Tokenized dataset saved successfully.\")"
      ],
      "metadata": {
        "id": "H7tN1Nfq_g_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if os.path.exists(processed_dataset_path):\n",
        "#     print(\"Loading preprocessed dataset from file...\")\n",
        "#     with open(processed_dataset_path, \"rb\") as f:\n",
        "#         tokenized_datasets = pickle.load(f)\n",
        "#     print(\"Preprocessed dataset loaded successfully.\")\n",
        "# else:\n",
        "#     print(\"Preprocessing dataset...\")\n",
        "#     tokenized_datasets = []\n",
        "#     for index, row in dataset.iterrows():\n",
        "#         tokenized_data = preprocess_function(row)\n",
        "#         tokenized_datasets.append(tokenized_data)\n",
        "#         # 중간중간 저장하여 RAM 용량 절약\n",
        "#         if (index + 1) % 1000 == 0:  # 매 1000개씩 저장\n",
        "#             with open(processed_dataset_path, \"wb\") as f:\n",
        "#                 pickle.dump(tokenized_datasets, f)\n",
        "#             print(f\"Saved {index + 1} tokenized examples so far...\")\n",
        "\n",
        "#     # 최종 저장\n",
        "#     with open(processed_dataset_path, \"wb\") as f:\n",
        "#         pickle.dump(tokenized_datasets, f)\n",
        "#     print(\"Tokenized dataset saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "collapsed": true,
        "id": "lCLYdrt9TDqI",
        "outputId": "7b75218e-5d56-442a-ec6f-2ba4f8a69176"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DatasetDict' object has no attribute 'iterrows'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3b61f9d2aa61>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessing dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtokenized_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtokenized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'iterrows'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 전처리\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "M3aUiupUVrcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#학습 및 저장\n"
      ],
      "metadata": {
        "id": "ccWT4lazgI4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint_path = \"/content/drive/MyDrive/summarizer/checkpoints\""
      ],
      "metadata": {
        "id": "fh3QxRgZgK3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 학습 설정\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"/content/drive/MyDrive/Colab Notebooks/Pegasus_요약모델/results\",\n",
        "#     evaluation_strategy=\"epoch\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=2,  # 배치 크기 감소\n",
        "#     per_device_eval_batch_size=2,  # 배치 크기 감소\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir=\"/content/drive/MyDrive/Colab Notebooks/Pegasus_요약모델/logs\",\n",
        "#     logging_steps=10,\n",
        "#     save_total_limit=2,\n",
        "# )\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/Pegasus_요약모델/results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,  # 배치 크기 감소\n",
        "    per_device_eval_batch_size=2,  # 배치 크기 감소\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/content/drive/MyDrive/Colab Notebooks/Pegasus_요약모델/logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "3D_BpNqcVvMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 트레이너 설정\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=tokenized_datasets[\"train\"],\n",
        "#     eval_dataset=tokenized_datasets[\"validation\"] if \"validation\" in tokenized_datasets else None,\n",
        "#     tokenizer=tokenizer,\n",
        "# )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"] if \"validation\" in tokenized_datasets else None,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "iUSLeFUubVIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "CaXqgmcRbgmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_path = \"/content/drive/MyDrive/summarizer/final_model\"\n",
        "trainer.save_model(final_model_path)\n",
        "print(\"Final model saved successfully.\")"
      ],
      "metadata": {
        "id": "90SamxkBggHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "model = T5ForConditionalGeneration.from_pretrained(final_model_path)\n",
        "print(\"Final model loaded successfully.\")"
      ],
      "metadata": {
        "id": "ulxtc1rXgh0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#테스트 요약"
      ],
      "metadata": {
        "id": "UdOMaW__glI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 시작 및 요약 결과 확인\n",
        "def summarize_article(article_text):\n",
        "    inputs = tokenizer(article_text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "    summary_ids = model.generate(**inputs, max_length=60, num_beams=5, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "ZReqxiA4bZpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용자 입력 문장을 요약\n",
        "article_text = \"여기다가 입력\"\n",
        "summary = summarize_article(article_text)\n",
        "print(\"\\nSummary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "rmwCM7XSH0mf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}