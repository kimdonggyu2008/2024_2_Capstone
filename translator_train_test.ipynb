{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP4qSkC7sEmB/ESbqOekKPn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9ad78317366465bb770d0fe0e18d812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99597df9e7e74ae68569a110a1e312a1",
              "IPY_MODEL_477ed4d481ba486081080fd2ef7261bd",
              "IPY_MODEL_951949aa37ef4489a01dffcea090ca8e"
            ],
            "layout": "IPY_MODEL_788ca56eb05947e7ac941448c69c9833"
          }
        },
        "99597df9e7e74ae68569a110a1e312a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3c4118e74544d02b39b342324a27985",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c1b2d83d6ab44d8d96b1769f67b9463f",
            "value": "Map:â€‡100%"
          }
        },
        "477ed4d481ba486081080fd2ef7261bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fee84acc08ef4b6da751c51d98c2e427",
            "max": 138887,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5abb3c3a331544cf818a1c1b44a9fb3e",
            "value": 138887
          }
        },
        "951949aa37ef4489a01dffcea090ca8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28fb2416a435408c932a2910c267c655",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_23d6adc67bb44d7194f7c8334c5ece3d",
            "value": "â€‡138887/138887â€‡[01:09&lt;00:00,â€‡2049.74â€‡examples/s]"
          }
        },
        "788ca56eb05947e7ac941448c69c9833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3c4118e74544d02b39b342324a27985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1b2d83d6ab44d8d96b1769f67b9463f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fee84acc08ef4b6da751c51d98c2e427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5abb3c3a331544cf818a1c1b44a9fb3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28fb2416a435408c932a2910c267c655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23d6adc67bb44d7194f7c8334c5ece3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/2024_2_Capstone/blob/main/translator_train_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ë²ˆì—­ ëª¨ë¸(helsinki) í•™ìŠµ ë° ì¶”ë¡  ì½”ë“œ"
      ],
      "metadata": {
        "id": "jLM6ZYjbE5Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVAebtHELUG0",
        "outputId": "3f5dda44-45ac-4c97-8a70-8ff2c7ee2364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjzhwiQkOhAZ",
        "outputId": "0b7b060c-aeae-45c7-a07f-f4a81437b0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import pickle\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Lx27sdP0MabO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ë°ì´í„° ë³‘í•© ë° í† í¬ë‚˜ì´ì €, ì²˜ë¦¬(í•„ìš”ì‹œ ì‚¬ìš©)"
      ],
      "metadata": {
        "id": "ZhqjyRD8TcCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. ë°ì´í„°ì…‹ ë¡œë“œ (ì˜ˆì‹œ ë°ì´í„°ì…‹)\n",
        "df = pd.read_csv('/content/drive/MyDrive/translator/data/train.csv')#train.csvíŒŒì¼ ìˆìœ¼ë©´ ë£¨íŠ¸ ë³€ê²½\n",
        "dataset = Dataset.from_pandas(df)\n"
      ],
      "metadata": {
        "id": "gqWBKk6mNHZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # combined_dfë¥¼ Datasetìœ¼ë¡œ ë³€í™˜\n",
        "# dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# # train, test ë°ì´í„°ì…‹ ë¶„ë¦¬ (ì˜ˆì‹œë¡œ 10%ë¥¼ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„ë¦¬)\n",
        "# train_test = dataset.train_test_split(test_size=0.1)\n",
        "# datasets = DatasetDict({\n",
        "#     'train': train_test['train'],\n",
        "#     'test': train_test['test']\n",
        "# })\n",
        "\n",
        "# # 4. ì „ì²˜ë¦¬ í•¨ìˆ˜ (í•œêµ­ì–´ -> ì˜ì–´ ë²ˆì—­)\n",
        "# def preprocess_function(examples):\n",
        "#     inputs = examples['ko']  # 'ko' ì—´ì—ì„œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´\n",
        "#     targets = examples['en']  # 'en' ì—´ì—ì„œ ì˜ì–´ ë²ˆì—­ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´\n",
        "#     model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     # íƒ€ê²Ÿ í…ìŠ¤íŠ¸ë„ í† í¬ë‚˜ì´ì§•\n",
        "#     labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs\n",
        "\n",
        "# # # ë°ì´í„°ì…‹ì„ ì „ì²˜ë¦¬ ë° í† í¬ë‚˜ì´ì§• (datasetsê°€ ì´ë¯¸ ë¡œë“œëœ ìƒíƒœì—ì„œ)\n",
        "# tokenized_datasets = datasets.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "jVUeMX-tNIPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ì…‹ì„ 4ê°œë¡œ ë‚˜ëˆ”\n",
        "\n",
        "\n",
        "# # 5. 4ê°œì˜ íŒŒíŠ¸ë¡œ ë¶„í• í•˜ê¸° ìœ„í•´ ì´ ë°ì´í„°ì…‹ í¬ê¸° ê³„ì‚°\n",
        "# dataset_length = len(tokenized_datasets['train'])\n",
        "\n",
        "# #íŒŒì¼ì„ 4ê°œë¡œ ë‚˜ëˆ„ì–´ì„œ ì €ì¥, ë‚˜ì¤‘ì— ê°ê° ë¶ˆëŸ¬ì™€ì„œ ì—í­ë§ˆë‹¤ ëŒë ¤ì•¼í• ë“¯\n",
        "# split_size = dataset_length // 4\n",
        "\n",
        "# # 7. ê° íŒŒíŠ¸ë¥¼ pkl íŒŒì¼ë¡œ ì €ì¥\n",
        "# for i in range(4):\n",
        "#     start_idx = i * split_size\n",
        "#     end_idx = (i + 1) * split_size if i < 3 else dataset_length  # ë§ˆì§€ë§‰ íŒŒíŠ¸ëŠ” ëê¹Œì§€\n",
        "\n",
        "#     part_dataset = tokenized_datasets['train'].select(range(start_idx, end_idx))\n",
        "\n",
        "#     output_file = f\"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/translator/processed_dataset_part_{i+1}.pkl\"\n",
        "#     with open(output_file, 'wb') as f:\n",
        "#         pickle.dump(part_dataset, f)\n",
        "\n",
        "#     print(f\"Tokenized dataset part {i+1} saved to {output_file}\")"
      ],
      "metadata": {
        "id": "HcPSinmf4OXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#í•™ìŠµ ì„¸íŒ…(í† í¬ë‚˜ì´ì§• ëœ ë°ì´í„° ë°›ìŒ)\n"
      ],
      "metadata": {
        "id": "Y-enH3CPTkb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ì„¤ì •, ì²˜ìŒ í•™ìŠµì‹œí‚¬ë•Œ, ì˜¨ë¼ì¸ ëª¨ë¸ ë°›ì•„ì™€ì„œ ì‚¬ìš©\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oiEwZwd3JrgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ì„¤ì •, ë¡œì»¬ë¡œ ë¶ˆëŸ¬ì™€ì„œ í•™ìŠµì‹œí‚¤ëŠ” ê²½ìš°\n",
        "model_name=\"/content/drive/MyDrive/translator/model_part_1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O9MPP5SH5uF",
        "outputId": "e2c59091-7bbd-4a37-d001-fbba59b530a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/translator/checkpoints\",  # ëª¨ë¸ ë° ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬\n",
        "    evaluation_strategy=\"epoch\",  # ì—í­ë§ˆë‹¤ í‰ê°€\n",
        "    save_strategy=\"epoch\",  # ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "    save_total_limit=3,  # ìµœê·¼ 3ê°œì˜ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€\n",
        "    logging_steps=100,  # ë¡œê·¸ë¥¼ ì¶œë ¥í•  ìŠ¤í… ê°„ê²©\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,  # ê° íŒŒì¼ë³„ë¡œ 5 ì—í­ì”© í•™ìŠµ\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,  # í•™ìŠµ ì¢…ë£Œ ì‹œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "    save_on_each_node=False,  # ëª¨ë“  ë…¸ë“œì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ (ë¶„ì‚° í•™ìŠµ ì‹œ)\n",
        "    logging_dir=\"/content/drive/MyDrive/translator/logging\",  # ë¡œê·¸ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬\n",
        ")\n"
      ],
      "metadata": {
        "id": "L1YSsDMVNgek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b6be7a-df84-4a15-e25d-9db71136f9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint_path = \"/content/drive/MyDrive/translator/checkpoints\""
      ],
      "metadata": {
        "id": "7TPKrnSqKjVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #ëª¨ë¸ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ìŒ\n",
        "#last_checkpoint = None\n",
        "#if os.path.isdir(model_checkpoint_path) and os.listdir(model_checkpoint_path):\n",
        "#    last_checkpoint = Trainer.get_last_checkpoint(model_checkpoint_path)\n",
        "#    if last_checkpoint is not None:\n",
        "#        print(f\"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤: {last_checkpoint}\")\n",
        "#    else:\n",
        "#        print(\"ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
        "#else:\n",
        " #   print(\"ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n"
      ],
      "metadata": {
        "id": "-OoBCklVKh3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#í•™ìŠµ(í•™ìŠµ íŒŒì¼ì€ ì—¬ëŸ¬ê°œë¡œ ë˜ì–´ìˆìŒ)"
      ],
      "metadata": {
        "id": "LTj35pyhEMht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 3. ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸\n",
        "# file_paths = [\n",
        "#     \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/translator/processed_dataset_part_1.pkl\",\n",
        "#     \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/translator/processed_dataset_part_2.pkl\",\n",
        "#     \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/translator/processed_dataset_part_3.pkl\",\n",
        "#     \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/translator/processed_dataset_part_4.pkl\"\n",
        "# ]"
      ],
      "metadata": {
        "id": "WWdZzSK05pTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_preprocessed_data(file_path):\n",
        "#     with open(file_path, 'rb') as f:\n",
        "#         tokenized_datasets = pickle.load(f)\n",
        "#     return tokenized_datasets"
      ],
      "metadata": {
        "id": "-BWsdHUzk40M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_preprocessed_data(file_path, split_ratio=0.8):\n",
        "#     with open(file_path, 'rb') as f:\n",
        "#         tokenized_datasets = pickle.load(f)\n",
        "\n",
        "#     # í•™ìŠµ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë‚˜ëˆ” (default: 80% train, 20% eval)\n",
        "#     train_size = int(split_ratio * len(tokenized_datasets))\n",
        "#     train_dataset = tokenized_datasets[:train_size]\n",
        "#     eval_dataset = tokenized_datasets[train_size:]\n",
        "\n",
        "#     return train_dataset, eval_dataset\n"
      ],
      "metadata": {
        "id": "6D-6Ecu4csnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from datasets import DatasetDict\n",
        "\n",
        "def load_preprocessed_data(file_path, split_ratio=0.8):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        tokenized_datasets = pickle.load(f)\n",
        "\n",
        "    # tokenized_datasetsê°€ Dataset í˜•ì‹ì¼ ê²½ìš° train_test_split ì‚¬ìš©\n",
        "    if isinstance(tokenized_datasets, DatasetDict):\n",
        "        split_data = tokenized_datasets.train_test_split(test_size=(1 - split_ratio))\n",
        "        train_dataset = split_data['train']\n",
        "        eval_dataset = split_data['test']\n",
        "    else:\n",
        "        # ë¦¬ìŠ¤íŠ¸ë‚˜ ë°°ì—´ì¸ ê²½ìš° ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ ë‚˜ëˆ”\n",
        "        train_size = int(split_ratio * len(tokenized_datasets))\n",
        "        train_dataset = tokenized_datasets[:train_size]\n",
        "        eval_dataset = tokenized_datasets[train_size:]\n",
        "\n",
        "    return train_dataset, eval_dataset\n"
      ],
      "metadata": {
        "id": "w_YBXCIMECIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokenized_data(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        tokenized_datasets = pickle.load(f)\n",
        "    return tokenized_datasets"
      ],
      "metadata": {
        "id": "HApeA9xfFQwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import pickle\n",
        "\n",
        "# ì „ì²˜ë¦¬ í•¨ìˆ˜ (ë™ì¼í•˜ê²Œ ì‚¬ìš©)\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['ko']  # 'ko' ì—´ì—ì„œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´\n",
        "    targets = examples['en']  # 'en' ì—´ì—ì„œ ì˜ì–´ ë²ˆì—­ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # íƒ€ê²Ÿ í…ìŠ¤íŠ¸ë„ í† í¬ë‚˜ì´ì§•\n",
        "    labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"/content/drive/MyDrive/translator/checkpoints\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# 6ê°œì˜ ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ í•™ìŠµ ì§„í–‰\n",
        "#í•œ ë°ì´í„°ì…‹ì˜ í•™ìŠµì´ ëë‚˜ë©´ í•˜ë‚˜ì”© ì§€ìš°ê³  ê¸¸ì´ ì¤„ì´ê¸° 1~7 -> 2~7 -> 3~7...\n",
        "for i in range(2, 7):\n",
        "    print(f\"Loading dataset part {i}...\")\n",
        "\n",
        "    # ì €ì¥ëœ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "    with open(f\"/content/drive/MyDrive/translator/data/part_{i}.pkl\", \"rb\") as f:\n",
        "        part = pickle.load(f)\n",
        "\n",
        "    # íŒŒíŠ¸ë§ˆë‹¤ ì „ì²˜ë¦¬ ë° í† í°í™”\n",
        "    tokenized_part = part.map(preprocess_function, batched=True)\n",
        "\n",
        "    # Trainer ì„¤ì •\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_part,  # í˜„ì¬ íŒŒíŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ\n",
        "        eval_dataset=tokenized_part,   # í‰ê°€ë„ ë™ì¼í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„¤ì •\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # apií‚¤ëŠ” ì´ê±¸ë¡œ ë„£ìœ¼ë©´ ë¨ 513a1f0c050fa7f60a76b5232e904d8df397082e\n",
        "    # í•™ìŠµ ì‹œì‘\n",
        "    print(f\"Training on dataset part {i}...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"Training interrupted during part {i}. Last checkpoint saved.\")\n",
        "\n",
        "    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "    trainer.save_model(f\"/content/drive/MyDrive/translator/model/model_part_{i}\")\n",
        "    print(f\"Model checkpoint saved after part {i}\")\n"
      ],
      "metadata": {
        "id": "XJK2ovNrTXBL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457,
          "referenced_widgets": [
            "a9ad78317366465bb770d0fe0e18d812",
            "99597df9e7e74ae68569a110a1e312a1",
            "477ed4d481ba486081080fd2ef7261bd",
            "951949aa37ef4489a01dffcea090ca8e",
            "788ca56eb05947e7ac941448c69c9833",
            "e3c4118e74544d02b39b342324a27985",
            "c1b2d83d6ab44d8d96b1769f67b9463f",
            "fee84acc08ef4b6da751c51d98c2e427",
            "5abb3c3a331544cf818a1c1b44a9fb3e",
            "28fb2416a435408c932a2910c267c655",
            "23d6adc67bb44d7194f7c8334c5ece3d"
          ]
        },
        "collapsed": true,
        "outputId": "b55aed6b-6f15-4eb8-b2ae-8d1c61a8676e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset part 2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9ad78317366465bb770d0fe0e18d812",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/138887 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on dataset part 2...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241027_043948-atyvkul8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface/runs/atyvkul8' target=\"_blank\">/content/drive/MyDrive/translator/checkpoints</a></strong> to <a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface' target=\"_blank\">https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface/runs/atyvkul8' target=\"_blank\">https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface/runs/atyvkul8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4005' max='173610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4005/173610 31:59 < 22:35:45, 2.08 it/s, Epoch 0.12/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4070' max='173610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4070/173610 32:31 < 22:35:15, 2.08 it/s, Epoch 0.12/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # 1. CSV íŒŒì¼ ë¡œë“œ\n",
        "# df = pd.read_csv('/content/drive/MyDrive/translator/data/train.csv')\n",
        "\n",
        "# # 2. Huggingface Datasetìœ¼ë¡œ ë³€í™˜\n",
        "# dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# # 3. ë°ì´í„°ë¥¼ 4ê°œë¡œ ë‚˜ëˆ„ê¸°\n",
        "# split_ratio = [0.25, 0.25, 0.25, 0.25]  # ë°ì´í„°ë¥¼ 4ë“±ë¶„í•  ë¹„ìœ¨\n",
        "# datasets_split = dataset.train_test_split(test_size=0.75)  # 1/4 ë°ì´í„° ì¶”ì¶œ\n",
        "# part_1 = datasets_split['train']  # ì²« ë²ˆì§¸ 1/4\n",
        "# remaining_data = datasets_split['test']\n",
        "\n",
        "# # ë‚˜ë¨¸ì§€ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë‚˜ëˆ”\n",
        "# datasets_split = remaining_data.train_test_split(test_size=2/3)  # 3/4 ë‚¨ì€ ë°ì´í„°ì—ì„œ 1/4 ì¶”ì¶œ\n",
        "# part_2 = datasets_split['train']\n",
        "# remaining_data = datasets_split['test']\n",
        "\n",
        "# datasets_split = remaining_data.train_test_split(test_size=1/2)  # 2/4 ë‚¨ì€ ë°ì´í„°ì—ì„œ 1/4 ì¶”ì¶œ\n",
        "# part_3 = datasets_split['train']\n",
        "# part_4 = datasets_split['test']\n",
        "\n",
        "# # 4. ê° íŒŒíŠ¸ë§ˆë‹¤ 5 ì—í­ì”© í•™ìŠµ\n",
        "# parts = [part_1, part_2, part_3, part_4]\n",
        "\n",
        "# # ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# def preprocess_function(examples):\n",
        "#     inputs = examples['ko']  # 'ko' ì—´ì—ì„œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´\n",
        "#     targets = examples['en']  # 'en' ì—´ì—ì„œ ì˜ì–´ ë²ˆì—­ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´\n",
        "#     model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     # íƒ€ê²Ÿ í…ìŠ¤íŠ¸ë„ í† í¬ë‚˜ì´ì§•\n",
        "#     labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs"
      ],
      "metadata": {
        "id": "pq-cKp09Kvx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# # 6. ê° íŒŒíŠ¸ë¥¼ 5 ì—í­ì”© í•™ìŠµ\n",
        "# for i, part in enumerate(parts):\n",
        "#     print(f\"Training on part {i+1}...\")\n",
        "\n",
        "#     # íŒŒíŠ¸ë§ˆë‹¤ ì „ì²˜ë¦¬ ë° í† í°í™”\n",
        "#     tokenized_part = part.map(preprocess_function, batched=True)\n",
        "\n",
        "#     # Trainer ì„¤ì •\n",
        "#     trainer = Trainer(\n",
        "#         model=model,\n",
        "#         args=training_args,\n",
        "#         train_dataset=tokenized_part,  # í˜„ì¬ íŒŒíŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ\n",
        "#         eval_dataset=tokenized_part,   # í‰ê°€ë„ ë™ì¼í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„¤ì •\n",
        "#         data_collator=data_collator,\n",
        "#         tokenizer=tokenizer\n",
        "#     )\n",
        "\n",
        "#     # apií‚¤ëŠ” ì´ê±¸ë¡œ ë„£ìœ¼ë©´ ë¨ 513a1f0c050fa7f60a76b5232e904d8df397082e\n",
        "\n",
        "#     # í•™ìŠµ ì‹œì‘\n",
        "#     print(f\"Training on dataset part {i+1}...\")\n",
        "#     try:\n",
        "#         trainer.train()\n",
        "#     except KeyboardInterrupt:\n",
        "#         print(f\"Training interrupted during part {i+1}. Last checkpoint saved.\")\n",
        "\n",
        "#     # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "#     trainer.save_model(f\"/content/drive/MyDrive/translator/model_part_{i+1}\")\n",
        "#     print(f\"Model checkpoint saved after part {i+1}\")"
      ],
      "metadata": {
        "id": "vEGoKdKyNe5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, file_path in enumerate(file_paths):\n",
        "#     # ê° íŒŒíŠ¸ì˜ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "#     print(file_path)\n",
        "#     print(f\"Loading dataset part {i+1} from {file_path}\")\n",
        "#     #loaded_data = load_preprocessed_data(file_path)\n",
        "#     loaded_data = load_preprocessed_data(file_path)\n",
        "\n",
        "#     dataset = Dataset.from_dict({\n",
        "#         'input_ids': loaded_data[0]['input_ids'],\n",
        "#         'attention_mask': loaded_data[0]['attention_mask'],\n",
        "#         'labels': loaded_data[0]['labels']\n",
        "#     })\n",
        "#     # train_dataset = loaded_dataset['train']\n",
        "#     # eval_dataset = loaded_dataset['test']\n",
        "#     # split_data = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "#     # 6. ë°ì´í„° ì½œë ˆì´í„° ì„¤ì • (Seq2Seqì— ë§ëŠ” í˜•íƒœ)\n",
        "#     data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "#     #split_data = loaded_data.train_test_split(test_size=0.2)\n",
        "\n",
        "#     tokenized_part = part.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "#     # 7. Trainer ì„¤ì •\n",
        "#     trainer = Trainer(\n",
        "#         model=model,\n",
        "#         args=training_args,\n",
        "#         #train_dataset=train_dataset,  # í•™ìŠµ ë°ì´í„°ì…‹\n",
        "#         #eval_dataset=eval_dataset, # í‰ê°€ ë°ì´í„°ì…‹ ì¶”ê°€\n",
        "#         train_dataset=tokenized_part,  # í˜„ì¬ íŒŒíŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ\n",
        "#         eval_dataset=tokenized_part,\n",
        "#         data_collator=data_collator,\n",
        "#         tokenizer=tokenizer\n",
        "#     )\n",
        "\n",
        "#     # 8. í•™ìŠµ ì‹œì‘\n",
        "#     print(f\"Training on dataset part {i+1}...\")\n",
        "#     try:\n",
        "#         trainer.train()\n",
        "#     except KeyboardInterrupt:\n",
        "#         print(f\"Training interrupted during part {i+1}. Last checkpoint saved.\")\n",
        "\n",
        "#     # apií‚¤ëŠ” ì´ê±¸ë¡œ ë„£ìœ¼ë©´ ë¨ 513a1f0c050fa7f60a76b5232e904d8df397082e\n",
        "\n",
        "#     # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "#     trainer.save_model(f\"/content/drive/MyDrive/translator/model_part_{i+1}\")\n",
        "#     print(f\"Model checkpoint saved after part {i+1}\")\n"
      ],
      "metadata": {
        "id": "9fNSKcxK54Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/translator/final_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/translator/final_model\")\n",
        "print(\"Final model and tokenizer saved.\")"
      ],
      "metadata": {
        "id": "ljY1cX3IDxZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ì…ë ¥ ë° ì˜ˆì¸¡\n"
      ],
      "metadata": {
        "id": "6xCogH9V-u0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# 1. í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ (í•™ìŠµì´ ëœë‹¤ë©´ ë³€ê²½ í•„ìš”)\n",
        "translation_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
        "translation_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
        "\n",
        "input_text=\"what should i say to you?\"\n",
        "\n",
        "inputs = translation_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "translated = translation_model.generate(**inputs)\n",
        "translated_text = translation_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "# 6. ë²ˆì—­ ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"ë²ˆì—­ëœ ì˜ì–´ ë¬¸ì¥: {translated_text}\")\n"
      ],
      "metadata": {
        "id": "AVzeIKyx-yq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}